---
title: "Stat 302 Project 3 Part 2"
author: "Ian Renshaw"
date: "3/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```


# Introduction

This is the second part of Project 3 for STAT 302. This part of the project shows my new abilities in setting up a data analysis project pipeline. The analysis makes use of the my_rf_cv() function from earlier code. This analysis will compare the cross-validation error when using different numbers of folds.

```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
source("../Code/my_rf_cv.R")
```

```{r, echo = FALSE, eval = TRUE}
# Random forest cross-validation with 2 folds
fold_2 <- c(1:30)
for(i in 1:30) {
  fold_2[i] <- my_rf_cv(2)
}

# Random forest cross-validation with 5 folds
fold_5 <- c(1:30)
for(i in 1:30) {
  fold_5[i] <- my_rf_cv(5)
}

# Random forest cross-validation with 10 folds
fold_10 <- c(1:30)
for(i in 1:30) {
  fold_10[i] <- my_rf_cv(10)
}

# Create data frame for CVE to use for plotting
df <- data.frame(CVE = fold_2, folds = "2")
df <- rbind(df, data.frame(CVE = fold_5, folds = "5"))
df <- rbind(df, data.frame(CVE = fold_10, folds = "10"))

folds_boxplot <- ggplot(data = df, aes(x = folds, y = CVE)) +
  geom_boxplot() +
  labs(x = "Folds", y = "Cross-Validation Error", title = "CV error comparison")
```

### CVE error for different numbers of folds across 30 simulations:
2 folds:

`r fold_2`


5 folds:

`r fold_5`


10 folds:

`r fold_10`


### Boxplot comparing CVE for different numbers of folds across 30 simiulations:
```{r, echo = FALSE, eval = TRUE}
folds_boxplot
```


```{r, echo = FALSE, eval = TRUE}
sum_stats <- data.frame(folds = 2,
           min = summary(fold_2)[[1]],
           first_quartile = summary(fold_2)[[2]],
           median = summary(fold_2)[[3]],
           mean = summary(fold_2)[[4]],
           third_quartile = summary(fold_2)[[5]],
           max = summary(fold_2)[[6]],
           variance = var(fold_2),
           standard_deviation = sd(fold_2)) %>%
    rbind(data.frame(folds = 5,
           min = summary(fold_5)[[1]],
           first_quartile = summary(fold_5)[[2]],
           median = summary(fold_5)[[3]],
           mean = summary(fold_5)[[4]],
           third_quartile = summary(fold_2)[[5]],
           max = summary(fold_5)[[6]],
           variance = var(fold_5),
           standard_deviation = sd(fold_5))) %>%
    rbind(data.frame(folds = 10,
           min = summary(fold_10)[[1]],
           first_quartile = summary(fold_10)[[2]],
           median = summary(fold_10)[[3]],
           mean = summary(fold_10)[[4]],
           third_quartile = summary(fold_10)[[5]],
           max = summary(fold_10)[[6]],
           variance = var(fold_10),
           standard_deviation = sd(fold_10)))
```

Comparison of summary statistics for the CVE of different numbers of folds across 30 simiulations:
```{r echo = FALSE, eval = TRUE}
sum_stats
```

# Conclusion

In conclusion, through the second part of project 3, I have learned how to navigate a data analysis project pipeline. I set up a cross-validation analysis using the my_rf_cv function and adapted earlier code to fit the project pipeline. The my_rf_cv function is loaded from the Code folder and uses the my_penguin data from the Data folder. The generated output is stored in the Output folder, in the respective subfolders.
